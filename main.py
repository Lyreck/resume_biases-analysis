import sys
import os
import re
import time as t
import pandas as pd
from rich.progress import track #progress bar

from scripts.utils import init_logging_config #all logs are stored in the logs folder.

os.makedirs("logs", exist_ok=True)
init_logging_config(filename="logs/main.log")

import logging
logger = logging.getLogger(__name__)



from scripts import resume_generation, cv_job_parsing
# en gros dans le init je mets direct les packages que je veux importer :)))))))))))

# Should I create a class with attributes ? Idea to explore for better adaptability of the codebase?

#data_decoding c'est le fichier qui contient les données brutes
#data_desc c'est le fichier qui contient les générations en sortie de LLM
# il faudrait vérifier que le database_generation fait par dad an'est pas dépendant du découpage asso/company.

if __name__ == "__main__":
    generate_descriptions, generate_pdfs, generate_offers, parse_files = True, True, True, True # booleans that will ultimately be passed as parameters. Tells us which part of the code we want to execute.
    multiprocessing=True #whether to use multiprocessing when computing score, or not.
    ## First, a minimal example. 
    # I want to generate one resume and one job offer, 
    # and I want to analyze their score, do a t-tes and plot graphs.
    
    # To run, you need to specify the following parameters:
    source_data_filename = "data_decoding"#_test" # filepath to a .csv containing everything we need to generate resumes: names, genders, ethnicity, companies(tech, med, educ), associations, education fields.
    data_desc_filename = "data_desc"
    # This .csv should have the following columns: name,surname,british,volunteering,gender,tech_comp,med_comp,educ_comp,field_study
    # Convention: 1 is british, 0 is not. 1 is female, 0 is male.

    #############################################################################################
    ########### Generate descriptions with the LLM and store them in a .csv database. ###########
    #############################################################################################
    # this chunk of code could be almost put in a independent web app for people to fill in company name, association name, and field of education, and get a PDF with LLM-generated descriptions !!

    if generate_descriptions:

        resume_generation.generate_descriptions(source_data_filename, out_filename=data_desc_filename, model='mistral-small', tech=True, med=True, edu=True, asso=True)

    ## Once all descriptions have been generated by the LLM, we generate resumes.

    ## We also create a database that stores every information about every resume generated. Every resume is identified by Name+Surname+Company+Association, which is unique for each resume.
    data_decoding = pd.read_csv(f"scripts/resume_generation/data/{source_data_filename}.csv") # the data of job offers
    data_desc = pd.read_csv(f"scripts/resume_generation/data/{data_desc_filename}.csv")
    
    # Create the all the resumes possible (all combinations) for analysis.
    logging.info("Creating unified resume database")
    database_name = "unified_database" ####this should go to parameters and I have to think of a good name
    data_for_generation = resume_generation.create_resume_database(data_decoding, data_desc, out_filename="unified_database", to_csv=True)
    logging.info(f"Database created successfully at scripts/resume_generation/data/{database_name}.csv")




    #############################################################################################
    ########### Generate PDFs from the descriptions and the database. ########################### 
    ############################################################################################# 

    if generate_pdfs:

        os.makedirs("scripts/cv_job_parsing/data/resumes_pdf", exist_ok=True) # create output directory if it does not exist yet.)
        resume_generation.generate_pdfs(data_for_generation, generation_strategy='companies', out_directory="scripts/cv_job_parsing/data/resumes_pdf", verbose=False) #verbos

        #soucis Yasmine (test in Overleaf - tried removing CP and add larger footer, does not change anything. pdflatex?)
        # test different progress bars to have a nice looking one.
        # write docstrings for the generate_pdfs function
        # Relocate the generate_pdfs function ?

        #another interesting to-do:
            # - put in place a "report" system for people to report when PDFs look weird, or whn it does not render. This could help correct issues


    #############################################################################################
    ########### Generate PDFs from the job offers given (in PDF, or scraped). ###################
    ############################################################################################# 

    if generate_offers:
        pass #TOBEDONE

    #############################################################################################
    ########### Parse generated PDFs to JSON using Resume Matcher functions #####################
    ########### This part is specific to each ATS algorithm and may vary    #####################
    ###########            if we are to implement ofther ones               #####################
    #############################################################################################
    
    # if parse_files: #parse both resumes and job offers.
    #     #for the moment, we process all resumes without discrimination. I would like to eventually add parameters that allow to select certain experiments? Would help run on smaller machines.

    #     cv_job_parsing.pdf_to_json()

    #     # Ensure the output directories for the experiments exists
    #     os.makedirs("scripts/cv_job_parsing/data/score_dataframes", exist_ok=True)

    #     if not multiprocessing:
    #         scores, resume_pdf_names, non_generated_resumes = [],[],0
    #         for job_offer_json_name in os.listdir("scripts/cv_job_parsing/data/JobDescription"):
    #             job_offer = re.search(r"(?<=JobDescription-).+?(?=\.pdf)", job_offer_json_name)
    #             logging.info(f"Computing compatibility score of all Resumes with {job_offer}.")
                
    #             for resume_pdf_name in track(os.listdir("scripts/cv_job_parsing/data/Resumes")):
    #                 try:
    #                     resume_json_name = cv_job_parsing.fetch_resume(resume_pdf_name)
    #                     resume_pdf_names.append(resume_pdf_name)

    #                     scores.append(cv_job_parsing.extract_score(resume_json_name, job_offer_json_name))

    #                 except ValueError as e:
    #                     non_generated_resumes+=1 #if fetching fails and raises a ValueError, we count it as a non generated resume (it means it was in the database, but the .tex was bugged). 

    #             # Save the scores to a CSV file
    #             data = {"Resume": resume_pdf_names, "JobDescription": [job_offer_json_name for _ in resume_pdf_names], "Score": scores, "adapted": [1 for _ in resume_pdf_names]} #job_offer_json_name should change to the pdf file if we want to automate the whole thing and stay coherent with the key for the Resume database.
    #             df = pd.DataFrame(data=data)

    #             df.to_csv(f"scripts/cv_job_parsing/data/dataframes/{job_offer}.csv", index=False)
    #             logging.info(f"While fetching resume json files, we found {non_generated_resumes} non-generated resumes.")


    #     if multiprocessing:
    #         from multiprocessing import Pool, cpu_count

    #         # Prepare list of all (resume, job_description) pairs
    #         resume_list = os.listdir("Data/Processed/Resumes")
    #         job_list = os.listdir("Data/Processed/JobDescription")
    #         pairs = [(resume, job) for resume in resume_list for job in job_list]

    #         n = len(pairs)

    #         ###### Time estimation
    #         t0 = t.time()

    #         # Set up the pool - you can adjust the number of processes if needed
    #         with Pool(processes=cpu_count()-1) as pool:
    #             results = pool.map(cv_job_parsing.process_pair, pairs[:min(n,100)])

    #         elapsed_time = (t.time() - t0) / 3600
    #         logging.info(f"Time estimation to compute score for {n} pairs (estimation sample: {100}): {(elapsed_time / 100)*n:.2f} hours.")
    #         #####

    #         t0 = t.time()

    #         # Set up the pool - you can adjust the number of processes if needed
    #         with Pool(processes=cpu_count()-1) as pool:
    #             results = pool.map(cv_job_parsing.process_pair, pairs)

    #         elapsed_time = (t.time() - t0) / 3600
    #         logging.info(f"Finished scoring {n} pairs in {elapsed_time:.2f} hours.")

    #         # Convert results to DataFrame
    #         df = pd.DataFrame(results, columns=["Resume", "JobDescription", "Score"])
    #         df.to_csv("scripts/cv_job_parsing/data/dataframes/all_scores.csv", index=False)
